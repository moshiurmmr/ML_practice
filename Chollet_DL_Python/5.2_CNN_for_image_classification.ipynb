{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Image Classification\n",
    "This notebook builds a convolutional neural network (CNN) for classifying images. It classifies images of cats and dogs. The dataset consists of 4000 images of cats and dogs with 2000 images for each animal.It uses the images from the well known Kaggle dataset which can be found here, https://www.kaggle.com/c/dogs-vs-cats/data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for directory creating and for copying image files\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of downloaded dataset\n",
    "original_dataset_dir = \"/home/moshiur/Downloads/dogs-vs-cats/train\"\n",
    "# the directory where the dataset is stored for analysis\n",
    "base_dir = \"/home/moshiur/Research/Machine_Learning/ML_practice/Chollet_DL_Python/datasets/cats_and_dogs_small\"\n",
    "os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories for training, validation and test data\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories for training cat and dog pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories for validation cat and dog pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories for test cat and dog pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy next 500 dog images to test_cats_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training cat images:  1000\n",
      "Total training dog images:  1000\n",
      "Total validation cat images:  500\n",
      "Total test dog images:  500\n"
     ]
    }
   ],
   "source": [
    "# check the copy of images\n",
    "print('Total training cat images: ', len(os.listdir(train_cats_dir)))\n",
    "print('Total training dog images: ', len(os.listdir(train_dogs_dir)))\n",
    "print('Total validation cat images: ', len(os.listdir(validation_cats_dir)))\n",
    "print('Total test dog images: ', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imptensorflow.kerasd layers from keras (using Tensorflow 2.0)\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPool2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPool2D((2, 2)))\n",
    "# flatten the output of the MaxPool layer\n",
    "model.add(layers.Flatten())\n",
    "# a densely connected layer of 512 neurons\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "# single output neuron as the output of the network is binary i.e., is the image is of a dog or a cat?\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 36992)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               18940416  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 19,034,177\n",
      "Trainable params: 19,034,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# scale all images by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# training data (for training images) generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # target dir\n",
    "        train_dir,\n",
    "        # all images will be of size (150, 150)\n",
    "        target_size = (150, 150),\n",
    "        #batch size for images\n",
    "        batch_size = 20,\n",
    "        # binary labels since we use binary_crossentropy as the cost function\n",
    "        class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# validation data (for validation images) generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        # target dir\n",
    "        validation_dir,\n",
    "        # all images will be of size (150, 150)\n",
    "        target_size = (150, 150),\n",
    "        #batch size for images\n",
    "        batch_size = 20,\n",
    "        # binary labels since we use binary_crossentropy as the cost function\n",
    "        class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at the output of the train_generator\n",
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 210s 2s/step - loss: 0.7151 - acc: 0.5345 - val_loss: 0.6543 - val_acc: 0.6400\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.6368 - acc: 0.6380 - val_loss: 0.6094 - val_acc: 0.6750\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.5787 - acc: 0.6925 - val_loss: 0.5869 - val_acc: 0.6875\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.5223 - acc: 0.7480 - val_loss: 0.6444 - val_acc: 0.6275\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.4883 - acc: 0.7635 - val_loss: 0.5824 - val_acc: 0.7000\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 111s 1s/step - loss: 0.4510 - acc: 0.7860 - val_loss: 0.5555 - val_acc: 0.7050\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.4083 - acc: 0.8275 - val_loss: 0.6202 - val_acc: 0.6975\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.3730 - acc: 0.8345 - val_loss: 0.6317 - val_acc: 0.6775\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.3407 - acc: 0.8560 - val_loss: 0.6105 - val_acc: 0.6900\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.2979 - acc: 0.8880 - val_loss: 0.5826 - val_acc: 0.7150\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.2745 - acc: 0.8945 - val_loss: 0.5945 - val_acc: 0.7225\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.2401 - acc: 0.9045 - val_loss: 0.6076 - val_acc: 0.7325\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.2068 - acc: 0.9275 - val_loss: 0.6467 - val_acc: 0.6925\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.1854 - acc: 0.9355 - val_loss: 0.6796 - val_acc: 0.7150\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.1550 - acc: 0.9460 - val_loss: 0.7068 - val_acc: 0.6900\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.1293 - acc: 0.9575 - val_loss: 0.7557 - val_acc: 0.7475\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.1105 - acc: 0.9670 - val_loss: 0.7271 - val_acc: 0.7175\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.0830 - acc: 0.9770 - val_loss: 0.7611 - val_acc: 0.7250\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.0711 - acc: 0.9835 - val_loss: 0.8116 - val_acc: 0.6950\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.0646 - acc: 0.9840 - val_loss: 0.8424 - val_acc: 0.7450\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 108s 1s/step - loss: 0.0533 - acc: 0.9860 - val_loss: 0.8324 - val_acc: 0.7350\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.0391 - acc: 0.9915 - val_loss: 0.8817 - val_acc: 0.7225\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.0360 - acc: 0.9920 - val_loss: 0.9528 - val_acc: 0.7275\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.0374 - acc: 0.9875 - val_loss: 0.9570 - val_acc: 0.7475\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.0281 - acc: 0.9925 - val_loss: 1.0244 - val_acc: 0.7150\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.0255 - acc: 0.9925 - val_loss: 0.9971 - val_acc: 0.7150\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.0143 - acc: 0.9965 - val_loss: 1.1462 - val_acc: 0.6950\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.0175 - acc: 0.9950 - val_loss: 1.0724 - val_acc: 0.7200\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.0091 - acc: 0.9970 - val_loss: 1.2755 - val_acc: 0.7225\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.0186 - acc: 0.9950 - val_loss: 1.1293 - val_acc: 0.7250\n"
     ]
    }
   ],
   "source": [
    "# fit the generator to the model\n",
    "history = model.fit_generator(\n",
    "    train_generator, steps_per_epoch=100, epochs=30,\n",
    "    validation_data = validation_generator, validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-3c26a2243cef>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-3c26a2243cef>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    plt.imshow(train_cats_dir/cat/cat.1.jpg)\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(train_cats_dir/cat/cat.1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
